{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "718e05f3-2e34-4d48-a39b-de8e7af4c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b951f68-1d10-4f17-8633-d83e53ec6f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/projects/ask_my_resume\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e837cb2-661d-416a-bbcd-935cbb1d7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Resume_Nida Madina Khan.docx\"\n",
    "# file_path = \"../data/resume.docx\"\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "resume_txt = extract_text_from_docx(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ffc8a2d-5dac-40ef-b81a-fea8d3606d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nida Madina Khan\n",
      "\n",
      "Work Experience\n",
      "Securiti, Karachi, Pakistan\t11/2024 – Current\n",
      "Data Scientist\t\n",
      "• Developed AI and Machine Learning solutions to process sensitive data securely, managing the full ML lifecycle from data collection and preparation to model training, evaluation, and production-readiness.\n",
      "• Built a CNN-based model to detect the header row location within semi-structured tabular files leveraging sentence embeddings and cosine similarity, achieving 96% accuracy, <250ms inference latency, and a broader context window compared to the previous system.\n",
      "• Integrated PyTorch models into a Java-based production pipeline for performance testing, collaborating closely with engineering teams to ensure seamless deployment.\n",
      "• Fine-tuned LLaMA 3 using PEFT (LoRA) to classify personal data (PD) types of columns in structured tables, achieving ~92% accuracy across 15+ PD types, and enabling classification of newly added PD types during inference.\n",
      "• Iteratively improved model accuracy through prompt-formatted instructions and data augmentation techniques, utilizing AI copilots like Claude Code to automate data transformations.\n",
      "\n",
      "Afiniti Software Solutions Ltd., Remote\t02/2020 – 10/2024\n",
      "Data Scientist II, Artificial Intelligence Team\t05/2022 – 10/2024\n",
      "Data Scientist, Artificial Intelligence Team\t05/2021 – 05/2022\n",
      "Junior Data Scientist, Artificial Intelligence Team\t02/2020 – 05/2021\n",
      "• Optimized call center interactions using machine learning techniques like Decision Trees and performance metrics to estimate customer lifetime value and agent impact, generating over $1 million in revenue per month for SKY UK.\n",
      "• Conducted extensive grid search experiments comprising of 2000+ models to find optimal hyper parameters.\n",
      "• Improved the efficiency of the ML pipeline by implementing strategies to reduce grid search time by 50%, and adding features to streamline and standardize the process of model logging.\n",
      "• Designed interactive dashboards using R and Python, resulting in streamlined tracking and visualization of KPIs.\n",
      "• Conducted real-time production monitoring using MySQL, leading to improved data quality for decision-making.\n",
      "• Used gradient-boosted decision trees (e.g., LightGBM) for predictive revenue forecasting and customer churn models.\n",
      "• Trained 10+ members across three clients on the ML pipeline, R programming and client business, contributing to an increase in project efficiency.\n",
      "\n",
      "Projects\n",
      "• End-to-end PySpark data pipeline to process large-scale taxi trip data. Used Spark local mode for prototyping and deployed the final pipeline on a single-node Dataproc cluster on GCP. Ingested Parquet-formatted data from GCS, standardized schemas, and wrote monthly aggregated metrics to BigQuery for reporting and analysis.\n",
      "• Machine Learning Pipeline Automation: Deployed an automated, end-to-end machine learning pipeline in R, encompassing data preprocessing, model training, validation, and deployment phases, greatly enhancing productivity.\n",
      "\n",
      "Skills\n",
      "Programming: Python, R, Java, SQL, Bash\n",
      "Data Engineering & MLOps: Spark, Docker, MLflow, Git, GCP\n",
      "Machine Learning & Modeling: Probability & Statistics, Deep Learning, LLM fine-tuning, Feature Engineering, Model Evaluation, Data Visualization (ggplot, Streamlit, Tableau)\n",
      "Languages: English (C2), Urdu (mother tongue)\n",
      "\n",
      "Education\n",
      "National University of Sciences and Technology, Islamabad, Pakistan\t2015 – 2019\n",
      "B.E. Electrical Engineering\n",
      "• Cumulative GPA: 3.83/4.00\n",
      "• Courses: Machine Learning, Probability & Statistics, Data Structures & Algorithms, Object-Oriented Programming\n"
     ]
    }
   ],
   "source": [
    "print(resume_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22c2907b-dd97-4b07-88cd-f3a927eb019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split text into sections using known headings\n",
    "def split_sections(text):\n",
    "    section_headers = [\"Work Experience\", \"Projects\", \"Skills\", \"Education\"]\n",
    "    section_map = {}\n",
    "    \n",
    "    # Regex split with lookahead\n",
    "    pattern = \"|\".join([re.escape(h) for h in section_headers])\n",
    "    splits = re.split(f\"(?=({pattern}))\", text)\n",
    "\n",
    "    # Group section headers with content\n",
    "    current_section = None\n",
    "    for item in splits:\n",
    "        if item in section_headers:\n",
    "            current_section = item\n",
    "            section_map[current_section] = \"\"\n",
    "        elif current_section:\n",
    "            section_map[current_section] += item.strip() + \"\\n\"\n",
    "    \n",
    "    return section_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77eb4d95-146f-4c7b-8cb5-1948f2fdde1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects\n",
      "• End-to-end PySpark data pipeline to process large-scale taxi trip data. Used Spark local mode for prototyping and deployed the final pipeline on a single-node Dataproc cluster on GCP. Ingested Parquet-formatted data from GCS, standardized schemas, and wrote monthly aggregated metrics to BigQuery for reporting and analysis.\n",
      "• Machine Learning Pipeline Automation: Deployed an automated, end-to-end machine learning pipeline in R, encompassing data preprocessing, model training, validation, and deployment phases, greatly enhancing productivity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(split_sections(resume_txt)['Projects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fb2be76-f5de-4c41-983e-8733ea924161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Chunk content inside each section\n",
    "def chunk_section_content(section_name, content):\n",
    "    chunks = []\n",
    "\n",
    "    # Split by double newlines or bullet points\n",
    "    raw_chunks = re.split(r\"\\n{2,}|•\", content)\n",
    "    for raw in raw_chunks:\n",
    "        text = raw.strip()\n",
    "        if len(text) > 30:  # skip very short lines\n",
    "            chunks.append({\n",
    "                \"id\": str(uuid4()),\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"section\": section_name\n",
    "                }\n",
    "            })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "749b130a-4d6d-4c27-a9ba-eb4020fe651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Combine everything\n",
    "def parse_resume_to_chunks(file_path):\n",
    "    resume_txt = extract_text_from_docx(file_path)\n",
    "    section_map = split_sections(resume_txt)\n",
    "\n",
    "    all_chunks = []\n",
    "    for section, content in section_map.items():\n",
    "        chunks = chunk_section_content(section, content)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43849f60-ce73-4953-bd71-42b41aecadc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk from Work Experience ---\n",
      "Work Experience\n",
      "Securiti, Karachi, Pakistan\t11/2024 – Current\n",
      "Data Scientist\n",
      "\n",
      "--- Chunk from Work Experience ---\n",
      "Developed AI and Machine Learning solutions to process sensitive data securely, managing the full ML lifecycle from data collection and preparation to model training, evaluation, and production-readiness.\n",
      "\n",
      "--- Chunk from Work Experience ---\n",
      "Built a CNN-based model to detect the header row location within semi-structured tabular files leveraging sentence embeddings and cosine similarity, achieving 96% accuracy, <250ms inference latency, and a broader context window compared to the previous system.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Run and preview\n",
    "file_path = \"Resume_Nida Madina Khan.docx\"\n",
    "resume_chunks = parse_resume_to_chunks(file_path)\n",
    "\n",
    "for chunk in resume_chunks[:3]:  # show sample output\n",
    "    print(f\"\\n--- Chunk from {chunk['metadata']['section']} ---\")\n",
    "    print(chunk['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f46cde42-161e-42a2-9fd4-58b275d6a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 chunks to resume.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_file = \"resume.json\"\n",
    "# output_file = \"../data/resume.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resume_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(resume_chunks)} chunks to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74385ccc-47cc-43ff-9131-e9a86da40ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load it back later\n",
    "# with open(\"parsed_resume_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     loaded_chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "566146fe-1801-4f1b-b917-10f878d731ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '79d65981-8a13-477c-aae8-78c33b5f3dbb',\n",
       "  'text': 'Work Experience\\nSecuriti, Karachi, Pakistan\\t11/2024 – Current\\nData Scientist',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': 'e6765195-3a24-4850-9f2c-4e7b62a8b680',\n",
       "  'text': 'Developed AI and Machine Learning solutions to process sensitive data securely, managing the full ML lifecycle from data collection and preparation to model training, evaluation, and production-readiness.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': 'e4fe21c0-43f7-451a-a323-5a5b26468bcd',\n",
       "  'text': 'Built a CNN-based model to detect the header row location within semi-structured tabular files leveraging sentence embeddings and cosine similarity, achieving 96% accuracy, <250ms inference latency, and a broader context window compared to the previous system.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': 'a3ebe70f-0779-482a-b4af-208cec3e20a4',\n",
       "  'text': 'Integrated PyTorch models into a Java-based production pipeline for performance testing, collaborating closely with engineering teams to ensure seamless deployment.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '519a9f86-98a5-4234-9434-08c416b381bb',\n",
       "  'text': 'Fine-tuned LLaMA 3 using PEFT (LoRA) to classify personal data (PD) types of columns in structured tables, achieving ~92% accuracy across 15+ PD types, and enabling classification of newly added PD types during inference.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '34271deb-a2b0-43ea-b8cb-a044606354cb',\n",
       "  'text': 'Iteratively improved model accuracy through prompt-formatted instructions and data augmentation techniques, utilizing AI copilots like Claude Code to automate data transformations.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '5a181d95-cf81-4d1b-8374-69e5f5501f7d',\n",
       "  'text': 'Afiniti Software Solutions Ltd., Remote\\t02/2020 – 10/2024\\nData Scientist II, Artificial Intelligence Team\\t05/2022 – 10/2024\\nData Scientist, Artificial Intelligence Team\\t05/2021 – 05/2022\\nJunior Data Scientist, Artificial Intelligence Team\\t02/2020 – 05/2021',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': 'a71cfa03-8618-4e73-97d9-d91898ae2ab4',\n",
       "  'text': 'Optimized call center interactions using machine learning techniques like Decision Trees and performance metrics to estimate customer lifetime value and agent impact, generating over $1 million in revenue per month for SKY UK.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': 'fe69e736-9977-4938-b16d-bf3cb0de4549',\n",
       "  'text': 'Conducted extensive grid search experiments comprising of 2000+ models to find optimal hyper parameters.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '85d2daed-6bba-4add-80b3-578db84b6e33',\n",
       "  'text': 'Improved the efficiency of the ML pipeline by implementing strategies to reduce grid search time by 50%, and adding features to streamline and standardize the process of model logging.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '137164d3-9eb7-4ab4-937b-6b856108ea2a',\n",
       "  'text': 'Designed interactive dashboards using R and Python, resulting in streamlined tracking and visualization of KPIs.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '77cd2827-0524-46a2-abfa-5c6eea307e62',\n",
       "  'text': 'Conducted real-time production monitoring using MySQL, leading to improved data quality for decision-making.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '60cc0170-dcc4-43eb-849d-57cb3b9e0cf9',\n",
       "  'text': 'Used gradient-boosted decision trees (e.g., LightGBM) for predictive revenue forecasting and customer churn models.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': 'fbae6d56-aece-4848-a8c9-d1a91b55c994',\n",
       "  'text': 'Trained 10+ members across three clients on the ML pipeline, R programming and client business, contributing to an increase in project efficiency.',\n",
       "  'metadata': {'section': 'Work Experience'}},\n",
       " {'id': '3012f0b1-4764-410c-99ab-08010b8edbb4',\n",
       "  'text': 'End-to-end PySpark data pipeline to process large-scale taxi trip data. Used Spark local mode for prototyping and deployed the final pipeline on a single-node Dataproc cluster on GCP. Ingested Parquet-formatted data from GCS, standardized schemas, and wrote monthly aggregated metrics to BigQuery for reporting and analysis.',\n",
       "  'metadata': {'section': 'Projects'}},\n",
       " {'id': 'd1fd9a4f-c175-4b74-b009-f86c392a2ed9',\n",
       "  'text': 'Machine Learning Pipeline Automation: Deployed an automated, end-to-end machine learning pipeline in R, encompassing data preprocessing, model training, validation, and deployment phases, greatly enhancing productivity.',\n",
       "  'metadata': {'section': 'Projects'}},\n",
       " {'id': 'a93177bc-f48f-44f4-b1e9-b655a70ce923',\n",
       "  'text': 'Skills\\nProgramming: Python, R, Java, SQL, Bash\\nData Engineering & MLOps: Spark, Docker, MLflow, Git, GCP\\nMachine Learning & Modeling: Probability & Statistics, Deep Learning, LLM fine-tuning, Feature Engineering, Model Evaluation, Data Visualization (ggplot, Streamlit, Tableau)\\nLanguages: English (C2), Urdu (mother tongue)',\n",
       "  'metadata': {'section': 'Skills'}},\n",
       " {'id': '98932c9c-481d-44a7-beed-aec909917961',\n",
       "  'text': 'Education\\nNational University of Sciences and Technology, Islamabad, Pakistan\\t2015 – 2019\\nB.E. Electrical Engineering',\n",
       "  'metadata': {'section': 'Education'}},\n",
       " {'id': '6009da0e-e1d0-4a1c-9b43-735aeec4982d',\n",
       "  'text': 'Courses: Machine Learning, Probability & Statistics, Data Structures & Algorithms, Object-Oriented Programming',\n",
       "  'metadata': {'section': 'Education'}}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ask_my_resume)",
   "language": "python",
   "name": "ask_my_resume"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
